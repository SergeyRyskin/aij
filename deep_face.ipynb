{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install deepface --user"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face Recognition and cropping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "backends = [\n",
    "    \"opencv\",\n",
    "    \"ssd\",\n",
    "    \"dlib\",\n",
    "    \"mtcnn\",\n",
    "    \"retinaface\",\n",
    "    \"mediapipe\"\n",
    "]\n",
    "\n",
    "default_backend = backends[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face = DeepFace.detectFace(\n",
    "    img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "    target_size=(224, 224),\n",
    "    detector_backend=default_backend\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face = DeepFace.detectFace(\n",
    "    img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "    target_size=(224, 224),\n",
    "    detector_backend=backends[5]  # mediapipe\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot all backend results in a single figure, use try except to avoid errors\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Face detection with different backends')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        try:\n",
    "            axs[i, j].imshow(DeepFace.detectFace(\n",
    "                img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                target_size=(224, 224),\n",
    "                detector_backend=backends[i * 3 + j]\n",
    "            ))\n",
    "            axs[i, j].set_title(backends[i * 3 + j])\n",
    "        except:\n",
    "            axs[i, j].set_title(backends[i * 3 + j] + \" not available\")\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face Verification and comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"VGG-Face\",\n",
    "    \"Facenet\",\n",
    "    \"Facenet512\",\n",
    "    \"OpenFace\",\n",
    "    \"DeepFace\",\n",
    "    \"DeepID\",\n",
    "    \"ArcFace\",\n",
    "    \"Dlib\",\n",
    "    \"SFace\",\n",
    "]\n",
    "\n",
    "default_model = models[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# face verification\n",
    "verification_result = DeepFace.verify(img1_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                                      img2_path=\"deep_face_db/keanu_reaves/keanu02.png\",\n",
    "                                      model_name=default_model,\n",
    "                                      detector_backend=default_backend,\n",
    "                                      distance_metric=\"cosine\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "fig.suptitle(\n",
    "    'Face verification with ' + default_model + ' and ' + default_backend + ' backend. Is the same person? ' + str(\n",
    "        verification_result[\"verified\"]) + '. Distance: ' + str(verification_result[\"distance\"]))\n",
    "\n",
    "# inverse BGR to RGB\n",
    "img1_path = \"deep_face_db/keanu_reaves/keanu01.png\"\n",
    "img1 = cv2.imread(img1_path)\n",
    "axs[0].imshow(img1[:, :, ::-1])\n",
    "axs[0].set_title(\"Keanu Reaves 1 (path: \" + img1_path + \")\")\n",
    "\n",
    "img2 = cv2.imread(\"deep_face_db/keanu_reaves/keanu02.png\")\n",
    "axs[1].imshow(img2[:, :, ::-1])\n",
    "axs[1].set_title(\"Keanu Reaves 2 (path: deep_face_db/keanu_reaves/keanu02.png)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Is the same person? \" + str(verification_result[\"verified\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "        # face verification\n",
    "        face_attributes = DeepFace.verify(img1_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                                          img2_path=\"deep_face_db/keanu_reaves/keanu02.png\",\n",
    "                                          model_name=model,\n",
    "                                          detector_backend=default_backend,\n",
    "                                          distance_metric=\"cosine\")\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\n",
    "            'Face verification with ' + model + ' and ' + default_backend + ' backend. Is the same person? ' + str(\n",
    "                face_attributes[\"verified\"]) + '. Distance: ' + str(face_attributes[\"distance\"]))\n",
    "\n",
    "        # inverse BGR to RGB\n",
    "        img1_path = \"deep_face_db/keanu_reaves/keanu01.png\"\n",
    "        img1 = cv2.imread(img1_path)\n",
    "        axs[0].imshow(img1[:, :, ::-1])\n",
    "        axs[0].set_title(\"Keanu Reaves 1 (path: \" + img1_path + \")\")\n",
    "\n",
    "        img2_path = \"deep_face_db/keanu_reaves/keanu02.png\"\n",
    "        img2 = cv2.imread(img2_path)\n",
    "        axs[1].imshow(img2[:, :, ::-1])\n",
    "        axs[1].set_title(\"Keanu Reaves 2 (path: \" + img2_path + \")\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(model + \" not available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = DeepFace.find(img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                    db_path=\"deep_face_db/\",\n",
    "                    model_name=default_model,\n",
    "                    detector_backend=default_backend,\n",
    "                    distance_metric=\"cosine\")\n",
    "\n",
    "dfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deepface is a hybrid face recognition package. It currently wraps many state-of-the-art face recognition models: VGG-Face , Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The default configuration uses VGG-Face model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#face verification\n",
    "face_comparison = DeepFace.verify(img1_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                                  img2_path=\"deep_face_db/scarlett johansson/scarlett02.png\",\n",
    "                                  model_name=models[0]\n",
    "                                  )\n",
    "\n",
    "#face recognition\n",
    "dfs = DeepFace.find(img_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                    db_path=\"deep_face_db/scarlett johansson/\",\n",
    "                    model_name=models[1]\n",
    "                    )\n",
    "\n",
    "#embeddings\n",
    "embedding_objs = DeepFace.represent(img_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                                    model_name=models[2]\n",
    "                                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete the model from memory\n",
    "del embedding_objs\n",
    "\n",
    "# if there is a model at deep_face_db/scarlett johansson/representations_facenet.pkl delete it too\n",
    "if os.path.isfile(\"deep_face_db/scarlett johansson/representations_facenet.pkl\"):\n",
    "    os.remove(\"deep_face_db/scarlett johansson/representations_facenet.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in models:\n",
    "\n",
    "    try:\n",
    "\n",
    "        img1_path = \"deep_face_db/scarlett johansson/scarlett01.png\"\n",
    "        img2_path = \"deep_face_db/scarlett johansson/scarlett05.png\"\n",
    "        face_comparison = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name=model)\n",
    "\n",
    "        # plot the result\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        axs[0].imshow(cv2.imread(img1_path)[:, :, ::-1])\n",
    "        axs[0].set_title(\"Scarlett Johansson 1 \\n(path: \" + img1_path + \")\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(cv2.imread(img2_path)[:, :, ::-1])\n",
    "        axs[1].set_title(\"Scarlett Johansson 2 \\n(path: \" + img2_path + \")\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        fig.suptitle('Face verification with ' + model + '. Is the same person? ' + str(\n",
    "            face_comparison[\"verified\"]) + '. Distance: ' + str(face_comparison[\"distance\"]))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(model + \" not available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "FaceNet, VGG-Face, ArcFace and Dlib are over-performing ones based on experiments. You can find out the scores of those models below on both Labeled Faces in the Wild and YouTube Faces in the Wild data sets declared by its creators."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Model           | \tLFW Score\t | YTF Score |\n",
    "|-----------------|-------------|-----------|\n",
    "| Facenet512      | \t99.65%     | \t-        |\n",
    "| SFace\t   | 99.60%      | \t-     |\n",
    "| ArcFace         | \t99.41%     | \t-        |\n",
    "| Dlib            | \t99.38%     | \t-        |\n",
    "| Facenet         | \t99.20%     | \t-        |\n",
    "| VGG-Face        | \t98.78%     | \t97.40%   |\n",
    "| Human-beings    | \t97.53%     | \t-        |\n",
    "| OpenFace        | \t93.80%     | \t-        |\n",
    "| DeepID          | \t-          | \t97.05%   |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Face recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RetinaFace and MTCNN seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.\n",
    "\n",
    "The performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That's why, alignment score of RetinaFace is high as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Facial Attribute Analysis\n",
    "\n",
    "Deepface supports facial attribute analysis as well. It is based on the VGG-Face model. You can find the list of attributes below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_attributes = DeepFace.analyze(\n",
    "    img_path=\"deep_face_db/scarlett johansson/scarlett05.png\",\n",
    "    detector_backend=\"opencv\",\n",
    "    align=True\n",
    ")\n",
    "\n",
    "face_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the attributes\n",
    "for attribute in face_attributes[0].keys():\n",
    "    print(attribute)\n",
    "\n",
    "# print the dominant emotion\n",
    "print(face_attributes[0]['dominant_emotion'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "imgs = glob(\"deep_face_db/yilmaz mustafa/*\")\n",
    "\n",
    "for img in imgs:\n",
    "    # plot emotions of the image\n",
    "    plt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs = glob(\"deep_face_db/yilmaz mustafa/*\")\n",
    "\n",
    "for img in imgs:\n",
    "\n",
    "    try:\n",
    "\n",
    "        emdf = DeepFace.analyze(img_path=img, actions=['emotion'], detector_backend=\"opencv\", align=True)\n",
    "\n",
    "        # Get the emotion data from the dictionary\n",
    "        emotion_data = emdf[0]['emotion']\n",
    "\n",
    "        # convert to dataframe\n",
    "        emotion_df = pd.DataFrame(emotion_data, index=[0])\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        axs[0].imshow(cv2.imread(img)[:, :, ::-1])\n",
    "        axs[0].set_title(\"Yilmaz Mustafa \\n(path: \" + img + \")\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].bar(emotion_df.columns, emotion_df.iloc[0])\n",
    "        axs[1].set_title(\"Emotion Analysis\")\n",
    "        axs[1].set_xlabel(\"Emotion\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(\"error occurred, possibly no face detected\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real Time Analysis\n",
    "\n",
    "You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if there is a model at deep_face_db/yilmaz mustafa/representations_facenet.pkl delete it too\n",
    "if os.path.isfile(\"deep_face_db/yilmaz mustafa/representations_facenet.pkl\"):\n",
    "    os.remove(\"deep_face_db/yilmaz mustafa/representations_facenet.pkl\")\n",
    "\n",
    "if os.path.isfile(\"deep_face_db/yilmaz mustafa/representations_vgg_face.pkl\"):\n",
    "    os.remove(\"deep_face_db/yilmaz mustafa/representations_vgg_face.pkl\")\n",
    "\n",
    "DeepFace.stream(db_path=\"deep_face_db/yilmaz mustafa\",\n",
    "                model_name=default_model,\n",
    "                detector_backend=default_backend,\n",
    "                distance_metric=\"cosine\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "default_backend = \"opencv\"\n",
    "default_model = \"Facenet\"\n",
    "\n",
    "\n",
    "# ask user to select a video file\n",
    "selection = input(\n",
    "    \"Select a video file to test the model (Default: Webcam (0) or enter the absolute path to a video file):\")\n",
    "\n",
    "if selection == \"0\" or selection == \"\" or selection is None:\n",
    "    cam = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cam = cv2.VideoCapture(selection)\n",
    "\n",
    "# set the resolution of the camera to full screen size\n",
    "# get the width and height of the screen\n",
    "width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# set the width and height, and UNSUCCESSFULLY set the exposure time\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "cam.set(cv2.CAP_PROP_FPS, 60)\n",
    "\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1 * cam.get(3)\n",
    "minH = 0.1 * cam.get(4)\n",
    "\n",
    "while True:\n",
    "    ret, img = cam.read()\n",
    "    img = cv2.flip(img, 1)  # Flip vertically\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # predict the emotion\n",
    "        emotion = DeepFace.analyze(img_path=img, actions=['emotion'], detector_backend=default_backend, align=True, silent=True)\n",
    "\n",
    "        # get the emotion data from the dictionary\n",
    "        emotion_data = emotion[0]['emotion']\n",
    "\n",
    "        # convert to dataframe\n",
    "        emotion_df = pd.DataFrame(emotion_data, index=[0])\n",
    "\n",
    "        # get the dominant emotion\n",
    "        emotion_label = emotion_df.idxmax(axis=1)[0]\n",
    "\n",
    "        # draw the emotion label at the bottom of the frame and display it such as \"Happy: 0.99, Sad: 0.01\"\n",
    "        cv2.putText(img, emotion_label + \": \" + str(emotion_df[emotion_label][0]), (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # if the emotion is happy, then draw a green rectangle around the face\n",
    "        if emotion_label == \"happy\":\n",
    "            cv2.rectangle(img, (int(minW), int(minH)), (int(width - minW), int(height - minH)), (0, 255, 0), 2)\n",
    "\n",
    "        # if the emotion is sad, then draw a red rectangle around the face\n",
    "        if emotion_label == \"sad\":\n",
    "            cv2.rectangle(img, (int(minW), int(minH)), (int(width - minW), int(height - minH)), (0, 0, 255), 2)\n",
    "\n",
    "        # if the emotion is angry, then draw a blue rectangle around the face\n",
    "        if emotion_label == \"angry\":\n",
    "            cv2.rectangle(img, (int(minW), int(minH)), (int(width - minW), int(height - minH)), (255, 0, 0), 2)\n",
    "\n",
    "        # if the emotion is neutral, then draw a yellow rectangle around the face\n",
    "        if emotion_label == \"neutral\":\n",
    "            cv2.rectangle(img, (int(minW), int(minH)), (int(width - minW), int(height - minH)), (0, 255, 255), 2)\n",
    "\n",
    "    except:\n",
    "        # draw error message if no face detected\n",
    "        cv2.putText(img, \"No face detected\", (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow('Emotion Recognition', img)\n",
    "\n",
    "    # press q to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
