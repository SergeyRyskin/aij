{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install deepface --user"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face Recognition and cropping"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "backends = [\n",
    "    \"opencv\",\n",
    "    \"ssd\",\n",
    "    \"dlib\",\n",
    "    \"mtcnn\",\n",
    "    \"retinaface\",\n",
    "    \"mediapipe\"\n",
    "]\n",
    "\n",
    "default_backend = backends[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face = DeepFace.detectFace(\n",
    "    img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "    target_size=(224, 224),\n",
    "    detector_backend=default_backend\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face = DeepFace.detectFace(\n",
    "    img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "    target_size=(224, 224),\n",
    "    detector_backend=backends[5]  # mediapipe\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot all backend results in a single figure, use try except to avoid errors\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Face detection with different backends')\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        try:\n",
    "            axs[i, j].imshow(DeepFace.detectFace(\n",
    "                img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                target_size=(224, 224),\n",
    "                detector_backend=backends[i * 3 + j]\n",
    "            ))\n",
    "            axs[i, j].set_title(backends[i * 3 + j])\n",
    "        except:\n",
    "            axs[i, j].set_title(backends[i * 3 + j] + \" not available\")\n",
    "            axs[i, j].axis('off')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Face Verification and comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"VGG-Face\",\n",
    "    \"Facenet\",\n",
    "    \"Facenet512\",\n",
    "    \"OpenFace\",\n",
    "    \"DeepFace\",\n",
    "    \"DeepID\",\n",
    "    \"ArcFace\",\n",
    "    \"Dlib\",\n",
    "    \"SFace\",\n",
    "]\n",
    "\n",
    "default_model = models[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# face verification\n",
    "verification_result = DeepFace.verify(img1_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                                      img2_path=\"deep_face_db/keanu_reaves/keanu02.png\",\n",
    "                                      model_name=default_model,\n",
    "                                      detector_backend=default_backend,\n",
    "                                      distance_metric=\"cosine\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "fig.suptitle(\n",
    "    'Face verification with ' + default_model + ' and ' + default_backend + ' backend. Is the same person? ' + str(\n",
    "        verification_result[\"verified\"]) + '. Distance: ' + str(verification_result[\"distance\"]))\n",
    "\n",
    "# inverse BGR to RGB\n",
    "img1_path = \"deep_face_db/keanu_reaves/keanu01.png\"\n",
    "img1 = cv2.imread(img1_path)\n",
    "axs[0].imshow(img1[:, :, ::-1])\n",
    "axs[0].set_title(\"Keanu Reaves 1 (path: \" + img1_path + \")\")\n",
    "\n",
    "img2 = cv2.imread(\"deep_face_db/keanu_reaves/keanu02.png\")\n",
    "axs[1].imshow(img2[:, :, ::-1])\n",
    "axs[1].set_title(\"Keanu Reaves 2 (path: deep_face_db/keanu_reaves/keanu02.png)\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Is the same person? \" + str(verification_result[\"verified\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    try:\n",
    "        # face verification\n",
    "        face_attributes = DeepFace.verify(img1_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                                          img2_path=\"deep_face_db/keanu_reaves/keanu02.png\",\n",
    "                                          model_name=model,\n",
    "                                          detector_backend=default_backend,\n",
    "                                          distance_metric=\"cosine\")\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        fig.suptitle(\n",
    "            'Face verification with ' + model + ' and ' + default_backend + ' backend. Is the same person? ' + str(\n",
    "                face_attributes[\"verified\"]) + '. Distance: ' + str(face_attributes[\"distance\"]))\n",
    "\n",
    "        # inverse BGR to RGB\n",
    "        img1_path = \"deep_face_db/keanu_reaves/keanu01.png\"\n",
    "        img1 = cv2.imread(img1_path)\n",
    "        axs[0].imshow(img1[:, :, ::-1])\n",
    "        axs[0].set_title(\"Keanu Reaves 1 (path: \" + img1_path + \")\")\n",
    "\n",
    "        img2_path = \"deep_face_db/keanu_reaves/keanu02.png\"\n",
    "        img2 = cv2.imread(img2_path)\n",
    "        axs[1].imshow(img2[:, :, ::-1])\n",
    "        axs[1].set_title(\"Keanu Reaves 2 (path: \" + img2_path + \")\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(model + \" not available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfs = DeepFace.find(img_path=\"deep_face_db/keanu_reaves/keanu01.png\",\n",
    "                    db_path=\"deep_face_db/\",\n",
    "                    model_name=default_model,\n",
    "                    detector_backend=default_backend,\n",
    "                    distance_metric=\"cosine\")\n",
    "\n",
    "dfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Deepface is a hybrid face recognition package. It currently wraps many state-of-the-art face recognition models: VGG-Face , Google FaceNet, OpenFace, Facebook DeepFace, DeepID, ArcFace, Dlib and SFace. The default configuration uses VGG-Face model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#face verification\n",
    "face_comparison = DeepFace.verify(img1_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                                  img2_path=\"deep_face_db/scarlett johansson/scarlett02.png\",\n",
    "                                  model_name=models[0]\n",
    "                                  )\n",
    "\n",
    "#face recognition\n",
    "dfs = DeepFace.find(img_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                    db_path=\"deep_face_db/scarlett johansson/\",\n",
    "                    model_name=models[1]\n",
    "                    )\n",
    "\n",
    "#embeddings\n",
    "embedding_objs = DeepFace.represent(img_path=\"deep_face_db/scarlett johansson/scarlett01.png\",\n",
    "                                    model_name=models[2]\n",
    "                                    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# delete the model from memory\n",
    "del embedding_objs\n",
    "\n",
    "# if there is a model at deep_face_db/scarlett johansson/representations_facenet.pkl delete it too\n",
    "if os.path.isfile(\"deep_face_db/scarlett johansson/representations_facenet.pkl\"):\n",
    "    os.remove(\"deep_face_db/scarlett johansson/representations_facenet.pkl\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in models:\n",
    "\n",
    "    try:\n",
    "\n",
    "        img1_path = \"deep_face_db/scarlett johansson/scarlett01.png\"\n",
    "        img2_path = \"deep_face_db/scarlett johansson/scarlett05.png\"\n",
    "        face_comparison = DeepFace.verify(img1_path=img1_path, img2_path=img2_path, model_name=model)\n",
    "\n",
    "        # plot the result\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        axs[0].imshow(cv2.imread(img1_path)[:, :, ::-1])\n",
    "        axs[0].set_title(\"Scarlett Johansson 1 \\n(path: \" + img1_path + \")\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(cv2.imread(img2_path)[:, :, ::-1])\n",
    "        axs[1].set_title(\"Scarlett Johansson 2 \\n(path: \" + img2_path + \")\")\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        fig.suptitle('Face verification with ' + model + '. Is the same person? ' + str(\n",
    "            face_comparison[\"verified\"]) + '. Distance: ' + str(face_comparison[\"distance\"]))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(model + \" not available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Face Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "FaceNet, VGG-Face, ArcFace and Dlib are over-performing ones based on experiments. You can find out the scores of those models below on both Labeled Faces in the Wild and YouTube Faces in the Wild data sets declared by its creators."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Model           | \tLFW Score\t | YTF Score |\n",
    "|-----------------|-------------|-----------|\n",
    "| Facenet512      | \t99.65%     | \t-        |\n",
    "| SFace\t   | 99.60%      | \t-     |\n",
    "| ArcFace         | \t99.41%     | \t-        |\n",
    "| Dlib            | \t99.38%     | \t-        |\n",
    "| Facenet         | \t99.20%     | \t-        |\n",
    "| VGG-Face        | \t98.78%     | \t97.40%   |\n",
    "| Human-beings    | \t97.53%     | \t-        |\n",
    "| OpenFace        | \t93.80%     | \t-        |\n",
    "| DeepID          | \t-          | \t97.05%   |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Face recognition models are actually CNN models and they expect standard sized inputs. So, resizing is required before representation. To avoid deformation, deepface adds black padding pixels according to the target size argument after detection and alignment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "RetinaFace and MTCNN seem to overperform in detection and alignment stages but they are much slower. If the speed of your pipeline is more important, then you should use opencv or ssd. On the other hand, if you consider the accuracy, then you should use retinaface or mtcnn.\n",
    "\n",
    "The performance of RetinaFace is very satisfactory even in the crowd as seen in the following illustration. Besides, it comes with an incredible facial landmark detection performance. Highlighted red points show some facial landmarks such as eyes, nose and mouth. That's why, alignment score of RetinaFace is high as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Facial Attribute Analysis\n",
    "\n",
    "Deepface supports facial attribute analysis as well. It is based on the VGG-Face model. You can find the list of attributes below."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "face_attributes = DeepFace.analyze(\n",
    "    img_path=\"deep_face_db/scarlett johansson/scarlett05.png\",\n",
    "    detector_backend=\"opencv\",\n",
    "    align=True\n",
    ")\n",
    "\n",
    "face_attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the attributes\n",
    "for attribute in face_attributes[0].keys():\n",
    "    print(attribute)\n",
    "\n",
    "# print the dominant emotion\n",
    "print(face_attributes[0]['dominant_emotion'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "imgs = glob(\"deep_face_db/yilmaz mustafa/*\")\n",
    "\n",
    "for img in imgs:\n",
    "    # plot emotions of the image\n",
    "    plt."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imgs = glob(\"deep_face_db/yilmaz mustafa/*\")\n",
    "\n",
    "for img in imgs:\n",
    "\n",
    "    try:\n",
    "\n",
    "        emdf = DeepFace.analyze(img_path=img, actions=['emotion'], detector_backend=\"opencv\", align=True)\n",
    "\n",
    "        # Get the emotion data from the dictionary\n",
    "        emotion_data = emdf[0]['emotion']\n",
    "\n",
    "        # convert to dataframe\n",
    "        emotion_df = pd.DataFrame(emotion_data, index=[0])\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "        axs[0].imshow(cv2.imread(img)[:, :, ::-1])\n",
    "        axs[0].set_title(\"Yilmaz Mustafa \\n(path: \" + img + \")\")\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].bar(emotion_df.columns, emotion_df.iloc[0])\n",
    "        axs[1].set_title(\"Emotion Analysis\")\n",
    "        axs[1].set_xlabel(\"Emotion\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    except:\n",
    "        print(\"error occurred, possibly no face detected\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Real Time Analysis\n",
    "\n",
    "You can run deepface for real time videos as well. Stream function will access your webcam and apply both face recognition and facial attribute analysis. The function starts to analyze a frame if it can focus a face sequentially 5 frames. Then, it shows results 5 seconds."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if there is a model at deep_face_db/yilmaz mustafa/representations_facenet.pkl delete it too\n",
    "if os.path.isfile(\"deep_face_db/yilmaz mustafa/representations_facenet.pkl\"):\n",
    "    os.remove(\"deep_face_db/yilmaz mustafa/representations_facenet.pkl\")\n",
    "\n",
    "if os.path.isfile(\"deep_face_db/yilmaz mustafa/representations_vgg_face.pkl\"):\n",
    "    os.remove(\"deep_face_db/yilmaz mustafa/representations_vgg_face.pkl\")\n",
    "\n",
    "DeepFace.stream(db_path=\"deep_face_db/yilmaz mustafa\",\n",
    "                model_name=default_model,\n",
    "                detector_backend=default_backend,\n",
    "                distance_metric=\"cosine\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 34\u001B[0m\n\u001B[0;32m     31\u001B[0m gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(img, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# predict the emotion\u001B[39;00m\n\u001B[1;32m---> 34\u001B[0m emotion \u001B[38;5;241m=\u001B[39m \u001B[43mDeepFace\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43memotion\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdetector_backend\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_backend\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malign\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;66;03m# get the emotion data from the dictionary\u001B[39;00m\n\u001B[0;32m     37\u001B[0m emotion_data \u001B[38;5;241m=\u001B[39m emotion[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\deepface\\DeepFace.py:336\u001B[0m, in \u001B[0;36manalyze\u001B[1;34m(img_path, actions, enforce_detection, detector_backend, align, silent)\u001B[0m\n\u001B[0;32m    333\u001B[0m img_gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mresize(img_gray, (\u001B[38;5;241m48\u001B[39m, \u001B[38;5;241m48\u001B[39m))\n\u001B[0;32m    334\u001B[0m img_gray \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(img_gray, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 336\u001B[0m emotion_predictions \u001B[38;5;241m=\u001B[39m \u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43memotion\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_gray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m, :]\n\u001B[0;32m    338\u001B[0m sum_of_predictions \u001B[38;5;241m=\u001B[39m emotion_predictions\u001B[38;5;241m.\u001B[39msum()\n\u001B[0;32m    340\u001B[0m obj[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memotion\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\keras\\engine\\training.py:2317\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[0;32m   2308\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m   2309\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   2310\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2311\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2314\u001B[0m             stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m   2315\u001B[0m         )\n\u001B[1;32m-> 2317\u001B[0m data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_data_handler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2319\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2320\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2321\u001B[0m \u001B[43m    \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2325\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2326\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2327\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2328\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2330\u001B[0m \u001B[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001B[39;00m\n\u001B[0;32m   2331\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1579\u001B[0m, in \u001B[0;36mget_data_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m   1577\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_cluster_coordinator\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1578\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _ClusterCoordinatorDataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 1579\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataHandler(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1259\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[0;32m   1256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_steps_per_execution \u001B[38;5;241m=\u001B[39m steps_per_execution\n\u001B[0;32m   1258\u001B[0m adapter_cls \u001B[38;5;241m=\u001B[39m select_data_adapter(x, y)\n\u001B[1;32m-> 1259\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapter \u001B[38;5;241m=\u001B[39m \u001B[43madapter_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1260\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1261\u001B[0m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1262\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1263\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1264\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1265\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1266\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshuffle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1269\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_multiprocessing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_multiprocessing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistribution_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_strategy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1271\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1272\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1274\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n\u001B[0;32m   1276\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:306\u001B[0m, in \u001B[0;36mTensorLikeDataAdapter.__init__\u001B[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001B[0m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m indices\n\u001B[0;32m    301\u001B[0m \u001B[38;5;66;03m# We prefetch a single element. Computing large permutations can take\u001B[39;00m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;66;03m# quite a while so we don't want to wait for prefetching over an epoch\u001B[39;00m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;66;03m# boundary to trigger the next permutation. On the other hand, too many\u001B[39;00m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;66;03m# simultaneous shuffles can contend on a hardware level and degrade all\u001B[39;00m\n\u001B[0;32m    305\u001B[0m \u001B[38;5;66;03m# performance.\u001B[39;00m\n\u001B[1;32m--> 306\u001B[0m indices_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mindices_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpermutation\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mprefetch(\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mslice_batch_indices\u001B[39m(indices):\n\u001B[0;32m    309\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \n\u001B[0;32m    311\u001B[0m \u001B[38;5;124;03m    This step can be accomplished in several ways. The most natural is\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    323\u001B[0m \u001B[38;5;124;03m      A Dataset of batched indices.\u001B[39;00m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2294\u001B[0m, in \u001B[0;36mDatasetV2.map\u001B[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001B[0m\n\u001B[0;32m   2291\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m deterministic \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m DEBUG_MODE:\n\u001B[0;32m   2292\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `deterministic` argument has no effect unless the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2293\u001B[0m                   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`num_parallel_calls` argument is specified.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2294\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mMapDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreserve_cardinality\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2295\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2296\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m ParallelMapDataset(\n\u001B[0;32m   2297\u001B[0m       \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2298\u001B[0m       map_func,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2301\u001B[0m       preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   2302\u001B[0m       name\u001B[38;5;241m=\u001B[39mname)\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5499\u001B[0m, in \u001B[0;36mMapDataset.__init__\u001B[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001B[0m\n\u001B[0;32m   5497\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_inter_op_parallelism \u001B[38;5;241m=\u001B[39m use_inter_op_parallelism\n\u001B[0;32m   5498\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preserve_cardinality \u001B[38;5;241m=\u001B[39m preserve_cardinality\n\u001B[1;32m-> 5499\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func \u001B[38;5;241m=\u001B[39m \u001B[43mstructured_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructuredFunctionWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   5500\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5501\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transformation_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5502\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   5503\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_legacy_function\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_legacy_function\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   5504\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_name \u001B[38;5;241m=\u001B[39m name\n\u001B[0;32m   5505\u001B[0m variant_tensor \u001B[38;5;241m=\u001B[39m gen_dataset_ops\u001B[38;5;241m.\u001B[39mmap_dataset(\n\u001B[0;32m   5506\u001B[0m     input_dataset\u001B[38;5;241m.\u001B[39m_variant_tensor,  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   5507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func\u001B[38;5;241m.\u001B[39mfunction\u001B[38;5;241m.\u001B[39mcaptured_inputs,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   5510\u001B[0m     preserve_cardinality\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_preserve_cardinality,\n\u001B[0;32m   5511\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_common_args)\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:263\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__\u001B[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[0;32m    256\u001B[0m       warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    257\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    258\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moption is set, this option does not apply to tf.data functions. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    259\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo force eager execution of tf.data functions, please use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    260\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    261\u001B[0m     fn_factory \u001B[38;5;241m=\u001B[39m trace_tf_function(defun_kwargs)\n\u001B[1;32m--> 263\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function \u001B[38;5;241m=\u001B[39m \u001B[43mfn_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# There is no graph to add in eager mode.\u001B[39;00m\n\u001B[0;32m    265\u001B[0m add_to_graph \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly()\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:226\u001B[0m, in \u001B[0;36mTracingCompiler.get_concrete_function\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_concrete_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    218\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m \n\u001B[0;32m    220\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;124;03m      `tf.Tensor` or `tf.TensorSpec`.\u001B[39;00m\n\u001B[0;32m    225\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 226\u001B[0m   concrete_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_concrete_function_garbage_collected(\n\u001B[0;32m    227\u001B[0m       \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    228\u001B[0m   concrete_function\u001B[38;5;241m.\u001B[39m_garbage_collector\u001B[38;5;241m.\u001B[39mrelease()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    229\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:192\u001B[0m, in \u001B[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    189\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_spec\u001B[38;5;241m.\u001B[39mvalidate_inputs_with_signature(args, kwargs)\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m--> 192\u001B[0m   concrete_function, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_concrete_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    193\u001B[0m   seen_names \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    194\u001B[0m   captured \u001B[38;5;241m=\u001B[39m object_identity\u001B[38;5;241m.\u001B[39mObjectIdentitySet(\n\u001B[0;32m    195\u001B[0m       concrete_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39minternal_captures)\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:157\u001B[0m, in \u001B[0;36mTracingCompiler._maybe_define_concrete_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m    154\u001B[0m   args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_signature\n\u001B[0;32m    155\u001B[0m   kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 157\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:360\u001B[0m, in \u001B[0;36mTracingCompiler._maybe_define_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m    357\u001B[0m   \u001B[38;5;66;03m# Only get placeholders for arguments, not captures\u001B[39;00m\n\u001B[0;32m    358\u001B[0m   args, kwargs \u001B[38;5;241m=\u001B[39m generalized_func_key\u001B[38;5;241m.\u001B[39m_placeholder_value()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m--> 360\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_concrete_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m graph_capture_container \u001B[38;5;241m=\u001B[39m concrete_function\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39m_capture_func_lib  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;66;03m# Maintain the list of all captures\u001B[39;00m\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:284\u001B[0m, in \u001B[0;36mTracingCompiler._create_concrete_function\u001B[1;34m(self, args, kwargs)\u001B[0m\n\u001B[0;32m    279\u001B[0m missing_arg_names \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    280\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (arg, i) \u001B[38;5;28;01mfor\u001B[39;00m i, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(missing_arg_names)\n\u001B[0;32m    281\u001B[0m ]\n\u001B[0;32m    282\u001B[0m arg_names \u001B[38;5;241m=\u001B[39m base_arg_names \u001B[38;5;241m+\u001B[39m missing_arg_names\n\u001B[0;32m    283\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m monomorphic_function\u001B[38;5;241m.\u001B[39mConcreteFunction(\n\u001B[1;32m--> 284\u001B[0m     \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mautograph_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_autograph_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marg_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_capture_by_value\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m    294\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function_attributes,\n\u001B[0;32m    295\u001B[0m     spec\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_spec,\n\u001B[0;32m    296\u001B[0m     \u001B[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001B[39;00m\n\u001B[0;32m    297\u001B[0m     \u001B[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001B[39;00m\n\u001B[0;32m    298\u001B[0m     \u001B[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001B[39;00m\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;66;03m# ConcreteFunction.\u001B[39;00m\n\u001B[0;32m    300\u001B[0m     shared_func_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m concrete_function\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1169\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001B[0m\n\u001B[0;32m   1167\u001B[0m   \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(op_return_value, ops\u001B[38;5;241m.\u001B[39mTensor), op_return_value\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m func_graph \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1169\u001B[0m   func_graph \u001B[38;5;241m=\u001B[39m \u001B[43mFuncGraph\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1170\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollections\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollections\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcapture_by_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcapture_by_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1171\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_graph, FuncGraph)\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m add_control_dependencies:\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:238\u001B[0m, in \u001B[0;36mFuncGraph.__init__\u001B[1;34m(self, name, collections, capture_by_value, structured_input_signature, structured_outputs)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    210\u001B[0m              name,\n\u001B[0;32m    211\u001B[0m              collections\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    212\u001B[0m              capture_by_value\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    213\u001B[0m              structured_input_signature\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    214\u001B[0m              structured_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    215\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Construct a new FuncGraph.\u001B[39;00m\n\u001B[0;32m    216\u001B[0m \n\u001B[0;32m    217\u001B[0m \u001B[38;5;124;03m  The graph will inherit its graph key, collections, seed, and distribution\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    236\u001B[0m \u001B[38;5;124;03m      information.\u001B[39;00m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 238\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mFuncGraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    239\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m name\n\u001B[0;32m    240\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minputs \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3212\u001B[0m, in \u001B[0;36mGraph.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   3208\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reduced_shape_cache \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m   3210\u001B[0m \u001B[38;5;66;03m# TODO(skyewm): fold as much of the above as possible into the C\u001B[39;00m\n\u001B[0;32m   3211\u001B[0m \u001B[38;5;66;03m# implementation\u001B[39;00m\n\u001B[1;32m-> 3212\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_c_graph \u001B[38;5;241m=\u001B[39m \u001B[43mc_api_util\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mScopedTFGraph\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3213\u001B[0m \u001B[38;5;66;03m# The C API requires all ops to have shape functions. Disable this\u001B[39;00m\n\u001B[0;32m   3214\u001B[0m \u001B[38;5;66;03m# requirement (many custom ops do not have shape functions, and we don't\u001B[39;00m\n\u001B[0;32m   3215\u001B[0m \u001B[38;5;66;03m# want to break these existing cases).\u001B[39;00m\n\u001B[0;32m   3216\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_c_graph\u001B[38;5;241m.\u001B[39mget() \u001B[38;5;28;01mas\u001B[39;00m c_graph:\n",
      "File \u001B[1;32m~\\Downloads\\mediapipe-keyboard\\mediapipe-sandbox\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\c_api_util.py:97\u001B[0m, in \u001B[0;36mScopedTFGraph.__init__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name):\n\u001B[0;32m     96\u001B[0m   \u001B[38;5;28msuper\u001B[39m(ScopedTFGraph, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m---> 97\u001B[0m       name, obj\u001B[38;5;241m=\u001B[39m\u001B[43mc_api\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTF_NewGraph\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, deleter\u001B[38;5;241m=\u001B[39mc_api\u001B[38;5;241m.\u001B[39mTF_DeleteGraph)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# names related to ids: example ==> Marcelo: id=1,  etc\n",
    "names = ['Anger', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# ask user to select a video file\n",
    "selection = input(\n",
    "    \"Select a video file to test the model (Default: Webcam (0) or enter the absolute path to a video file):\")\n",
    "\n",
    "if selection == \"0\" or selection == \"\" or selection is None:\n",
    "    cam = cv2.VideoCapture(0)\n",
    "else:\n",
    "    cam = cv2.VideoCapture(selection)\n",
    "\n",
    "# set the resolution of the camera to full screen size\n",
    "# get the width and height of the screen\n",
    "width = int(cam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "# set the width and height, and UNSUCCESSFULLY set the exposure time\n",
    "cam.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cam.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "cam.set(cv2.CAP_PROP_FPS, 60)\n",
    "\n",
    "# Define min window size to be recognized as a face\n",
    "minW = 0.1 * cam.get(3)\n",
    "minH = 0.1 * cam.get(4)\n",
    "\n",
    "while True:\n",
    "    ret, img = cam.read()\n",
    "    img = cv2.flip(img, 1)  # Flip vertically\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # predict the emotion\n",
    "    emotion = DeepFace.analyze(img_path=img, actions=['emotion'], detector_backend=default_backend, align=True, silent=True)\n",
    "\n",
    "    # get the emotion data from the dictionary\n",
    "    emotion_data = emotion[0]['emotion']\n",
    "\n",
    "    # convert to dataframe\n",
    "    emotion_df = pd.DataFrame(emotion_data, index=[0])\n",
    "\n",
    "    # get the dominant emotion\n",
    "    emotion_label = emotion_df.idxmax(axis=1)[0]\n",
    "\n",
    "    # draw the emotion label at the bottom of the frame and display it such as \"Happy: 0.99, Sad: 0.01\"\n",
    "    cv2.putText(img, emotion_label + \": \" + str(emotion_df[emotion_label][0]), (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # show the frame\n",
    "    cv2.imshow('Emotion Recognition', img)\n",
    "\n",
    "    # press q to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
